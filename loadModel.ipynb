{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer from 3d CNN\n",
    "\n",
    "the codes below needs review. Medyo makalat tong part na 'to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=examination_behavior-20F-216IS-16.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 70\u001b[0m\n\u001b[0;32m     67\u001b[0m     cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m     69\u001b[0m \u001b[38;5;66;03m# Load saved model\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexamination_behavior-20F-216IS-16.keras\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Initialize encoder with class labels\u001b[39;00m\n\u001b[0;32m     73\u001b[0m CLASSES \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNon-suspicious\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSuspicious\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:200\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File not found: filepath=examination_behavior-20F-216IS-16.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "# Predict with a sample video\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "\n",
    "def predict_real_time(video_path, model, encoder, img_size=(216, 216), interval=15, window_size=20):\n",
    "    \"\"\"\n",
    "    Predicts actions every `interval` seconds using 16 evenly spaced frames from the past 5 seconds.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_buffer = []\n",
    "    frame_idx = 0\n",
    "    last_pred_time = 0  # Last prediction time in video seconds\n",
    "\n",
    "    frames_needed = int(interval * fps)  \n",
    "\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"Total frames: {total_frames}, FPS: {fps}\")\n",
    "    print(f\"Frames per {interval}-second window: {frames_needed}, Sampling {window_size} frames\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize and normalize frame\n",
    "        frame = cv2.resize(frame, img_size)\n",
    "        frame = frame / 255.0\n",
    "        frame_buffer.append(frame)\n",
    "\n",
    "        # Keep only the last `frames_needed` frames\n",
    "        if len(frame_buffer) > frames_needed:\n",
    "            frame_buffer.pop(0)  # Remove the oldest frame\n",
    "\n",
    "        # Compute current video time\n",
    "        current_time = frame_idx / fps  # Convert frame index to seconds\n",
    "\n",
    "        # Predict every `interval` seconds in video time\n",
    "        if current_time >= last_pred_time + interval:\n",
    "            if len(frame_buffer) == frames_needed:  # Ensure we have enough frames\n",
    "                # evenly spaced frames\n",
    "                sampled_frames = np.array([frame_buffer[math.floor(i * len(frame_buffer) / window_size)] for i in range(window_size)])\n",
    "\n",
    "                for i, frame in enumerate(sampled_frames):\n",
    "                    cv2.imwrite(f\"debug_frame_{i}.jpg\", (frame * 255).astype(np.uint8))\n",
    "                    \n",
    "                # Add batch dimension\n",
    "                input_data = np.expand_dims(sampled_frames, axis=0)\n",
    "                pred = model.predict(input_data, verbose=0)\n",
    "                class_idx = np.argmax(pred)\n",
    "                predicted_label = encoder.inverse_transform([class_idx])[0]\n",
    "\n",
    "                # Display prediction with timestamp\n",
    "                minutes = int(current_time // 60)\n",
    "                seconds = int(current_time % 60)\n",
    "                print(f\"[{minutes:02d}:{seconds:02d}] Predicted Action: {predicted_label}\")\n",
    "\n",
    "                last_pred_time = current_time  # Update last prediction time\n",
    "        \n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Load saved model\n",
    "model = tf.keras.models.load_model(\"examination_behavior-20F-216IS-16.keras\")\n",
    "\n",
    "# Initialize encoder with class labels\n",
    "CLASSES = ['Non-suspicious', 'Suspicious']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(CLASSES)\n",
    "\n",
    "# Path to long video\n",
    "video_path = \"D:\\Recordings\\Recordings\\Part1.mp4\"\n",
    "\n",
    "# Predict actions every 5 seconds using 16 sampled frames\n",
    "predict_real_time(video_path, model, encoder, img_size=(216, 216), interval=15, window_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam started. Press 'q' to exit.\n",
      "[00:15] Predicted Action: Suspicious\n"
     ]
    }
   ],
   "source": [
    "# Predict with the webcam\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "import time\n",
    "\n",
    "def predict_real_time(model, encoder, img_size=(216, 216), interval=15, window_size=20):\n",
    "    \"\"\"\n",
    "    Predicts actions from the webcam every `interval` seconds using evenly spaced frames from the past `interval` seconds.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(0)  # Open webcam\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30  # Default to 30 FPS if unknown\n",
    "    frame_buffer = []\n",
    "    frame_idx = 0\n",
    "    last_pred_time = 0  # Track time in seconds\n",
    "\n",
    "    print(\"Webcam started. Press 'q' to exit.\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize and normalize frame\n",
    "        frame_resized = cv2.resize(frame, img_size)\n",
    "        frame_resized = frame_resized / 255.0\n",
    "        frame_buffer.append(frame_resized)\n",
    "\n",
    "        # Keep only the last `frames_needed` frames\n",
    "        frames_needed = int(interval * fps)  # Frames in past `interval` seconds\n",
    "        if len(frame_buffer) > frames_needed:\n",
    "            frame_buffer.pop(0)  # Remove oldest frame\n",
    "\n",
    "        # Compute current video time\n",
    "        current_time = frame_idx / fps  # Convert frame index to seconds\n",
    "\n",
    "        # Predict every `interval` seconds\n",
    "        if current_time >= last_pred_time + interval:\n",
    "            if len(frame_buffer) >= window_size:\n",
    "                # evenly spaced frames\n",
    "                sampled_frames = np.array([frame_buffer[math.floor(i * len(frame_buffer) / window_size)] for i in range(window_size)])\n",
    "\n",
    "                # Add batch dimension\n",
    "                input_data = np.expand_dims(sampled_frames, axis=0)\n",
    "                pred = model.predict(input_data, verbose=0)\n",
    "                class_idx = np.argmax(pred)\n",
    "                predicted_label = encoder.inverse_transform([class_idx])[0]\n",
    "\n",
    "                # Compute formatted timestamp\n",
    "                minutes = int(current_time // 60)\n",
    "                seconds = int(current_time % 60)\n",
    "                print(f\"[{minutes:02d}:{seconds:02d}] Predicted Action: {predicted_label}\")\n",
    "\n",
    "                last_pred_time += interval  # Move to the next prediction time\n",
    "\n",
    "        # Overlay timestamp on frame\n",
    "        timestamp_text = f\"Time: {time.strftime('%H:%M:%S')}\"\n",
    "        cv2.putText(frame, timestamp_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "        # Show webcam feed\n",
    "        try:\n",
    "            cv2.imshow(\"Webcam Feed\", frame)\n",
    "        except cv2.error:\n",
    "            pass  # Ignore errors if running in a non-GUI environment\n",
    "\n",
    "        # Exit on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "        frame_idx += 1  # Increase frame count\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Load saved model\n",
    "model = tf.keras.models.load_model(\"examination_behavior-20F-216IS-29.keras\")\n",
    "\n",
    "# Initialize encoder with class labels\n",
    "CLASSES = ['Non-suspicious', 'Suspicious']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(CLASSES)\n",
    "\n",
    "# Predict actions using the webcam\n",
    "predict_real_time(model, encoder, img_size=(216, 216), interval=15, window_size=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Non-suspicious (0.51)\n",
      "Prediction: Non-suspicious (0.51)\n",
      "Prediction: Non-suspicious (0.50)\n",
      "Prediction: Non-suspicious (0.50)\n",
      "Prediction: Suspicious (0.51)\n",
      "Prediction: Non-suspicious (0.50)\n",
      "Prediction: Suspicious (0.50)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 216\n",
    "FRAMES = 20\n",
    "CHANNELS = 1  # Grayscale\n",
    "CLASSES = ['Non-suspicious', 'Suspicious']\n",
    "SEGMENT_DURATION = 15  # seconds\n",
    "model_path = \"examination_behavior-20F-216IS-31-gray.keras\"\n",
    "\n",
    "# Load the model\n",
    "model = load_model(model_path)\n",
    "\n",
    "segment_frames = []\n",
    "segment_start_time = time.time()\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    frame = cv2.resize(frame, (IMG_SIZE, IMG_SIZE))\n",
    "    frame = frame / 255.0\n",
    "    frame = np.expand_dims(frame, axis=-1)  # Add channel dimension\n",
    "    return frame\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to grayscale\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Preprocess\n",
    "    processed = preprocess_frame(gray_frame)\n",
    "    segment_frames.append(processed)\n",
    "\n",
    "    current_time = time.time()\n",
    "    if current_time - segment_start_time >= SEGMENT_DURATION:\n",
    "        total = len(segment_frames)\n",
    "        if total >= FRAMES:\n",
    "            indices = np.linspace(0, total - 1, FRAMES).astype(int)\n",
    "            sampled_frames = np.array([segment_frames[i] for i in indices])\n",
    "            input_data = np.expand_dims(sampled_frames, axis=0)  # shape: (1, 20, 216, 216, 1)\n",
    "\n",
    "            # Predict\n",
    "            preds = model.predict(input_data, verbose=0)\n",
    "            label_idx = np.argmax(preds)\n",
    "            label = CLASSES[label_idx]\n",
    "            confidence = preds[0][label_idx]\n",
    "\n",
    "            print(f\"Prediction: {label} ({confidence:.2f})\")\n",
    "\n",
    "            # Annotate display\n",
    "            cv2.putText(gray_frame, f\"{label} ({confidence:.2f})\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                        (255), 2)\n",
    "\n",
    "        segment_frames = []\n",
    "        segment_start_time = current_time\n",
    "\n",
    "    # Show live grayscale frame\n",
    "    cv2.imshow(\"Live Suspicious Behavior Detection (Grayscale)\", gray_frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Haven't reviewed this one yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step\n",
      "Predicted Action: Grazing\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load saved model\n",
    "model = tf.keras.models.load_model(\"goat_action_3dcnn.h5\")\n",
    "IMG_SIZE = 64  # Resize frames to 64x64\n",
    "FRAMES = 16  # Fixed number of frames per video\n",
    "CHANNELS = 3  # RGB channels\n",
    "CLASSES = ['Running', 'Sitting', 'Grazing']\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(CLASSES)\n",
    "\n",
    "def extract_video_frames(video_path, num_frames, img_size):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)  # Select evenly spaced frames\n",
    "    \n",
    "    for i in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if i in frame_indices:\n",
    "            frame = cv2.resize(frame, (img_size, img_size))\n",
    "            frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if len(frames) < num_frames:  # Pad if not enough frames\n",
    "        frames.extend([frames[-1]] * (num_frames - len(frames)))\n",
    "    \n",
    "    return np.array(frames) if len(frames) == num_frames else None\n",
    "\n",
    "def predict_video(video_path, model, encoder):\n",
    "    video_data = extract_video_frames(video_path, FRAMES, IMG_SIZE)\n",
    "    \n",
    "    if video_data is None:\n",
    "        print(\"Error processing video.\")\n",
    "        return\n",
    "    \n",
    "    video_data = np.expand_dims(video_data / 255.0, axis=0)\n",
    "    prediction = model.predict(video_data)\n",
    "    \n",
    "    class_idx = np.argmax(prediction)\n",
    "    predicted_label = encoder.inverse_transform([class_idx])[0]\n",
    "    print(f\"Predicted Action: {predicted_label}\")\n",
    "\n",
    "# Test with a sample video\n",
    "predict_video(r\"Goat\\Train\\Grazing\\002(front).mp4\", model, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def predict_video_segments(video_path, model, encoder, window_size=16, stride=16, img_size=(64, 64)):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        model (tf.keras.Model): Trained 3D CNN model.\n",
    "        encoder (LabelEncoder): Encoder for decoding class labels.\n",
    "        window_size (int): Number of frames per prediction window.\n",
    "        stride (int): Number of frames to move between windows.\n",
    "        img_size (tuple): Resize dimensions for frames.\n",
    "\n",
    "    Returns:\n",
    "        list: List of (timestamp, predicted_action) tuples.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    predictions = []\n",
    "    frame_buffer = []\n",
    "    frame_idx = 0\n",
    "\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"Total frames: {total_frames}, FPS: {fps}\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Resize and normalize frame\n",
    "        frame = cv2.resize(frame, img_size)\n",
    "        frame = frame / 255.0\n",
    "        frame_buffer.append(frame)\n",
    "\n",
    "        # Predict when enough frames are collected\n",
    "        if len(frame_buffer) == window_size:\n",
    "            input_data = np.expand_dims(np.array(frame_buffer), axis=0)  # Shape: (1, window_size, H, W, C)\n",
    "            pred = model.predict(input_data, verbose=0)\n",
    "            class_idx = np.argmax(pred)\n",
    "            predicted_label = encoder.inverse_transform([class_idx])[0]\n",
    "\n",
    "            # Calculate timestamp\n",
    "            timestamp = frame_idx / fps\n",
    "            predictions.append((timestamp, predicted_label))\n",
    "\n",
    "            # Slide window\n",
    "            frame_buffer = frame_buffer[stride:]\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Display predictions\n",
    "    print(\"\\nPredicted Actions:\")\n",
    "    for timestamp, action in predictions:\n",
    "        minutes = int(timestamp // 60)\n",
    "        seconds = int(timestamp % 60)\n",
    "        print(f\"[{minutes:02d}:{seconds:02d}] Predicted Action: {action}\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# ----------------------------------\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load saved model\n",
    "model = tf.keras.models.load_model(\"goat_action_3dcnn.h5\")\n",
    "\n",
    "# Initialize encoder with class labels\n",
    "CLASSES = ['Running', 'Sitting', 'Grazing']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(CLASSES)\n",
    "\n",
    "# Path to long video\n",
    "video_path = r\"stitched_output.mp4\"\n",
    "\n",
    "# Predict actions in long video\n",
    "predict_video_segments(video_path, model, encoder, window_size=16, stride=16, img_size=(64, 64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "\n",
    "def predict_real_time(video_path, model, encoder, window_size=16, stride=8, img_size=(64, 64), interval=5):\n",
    "    \"\"\"\n",
    "    Predicts actions from a video stream in real-time every `interval` seconds.\n",
    "\n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        model (tf.keras.Model): Trained 3D CNN model.\n",
    "        encoder (LabelEncoder): Encoder for decoding class labels.\n",
    "        window_size (int): Number of frames per prediction window.\n",
    "        stride (int): Number of frames to move between windows.\n",
    "        img_size (tuple): Resize dimensions for frames.\n",
    "        interval (int): Time interval (in seconds) for predictions.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_buffer = []\n",
    "    frame_idx = 0\n",
    "    last_pred_time = time.time()\n",
    "    \n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"Total frames: {total_frames}, FPS: {fps}\")\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize and normalize frame\n",
    "        frame = cv2.resize(frame, img_size)\n",
    "        frame = frame / 255.0\n",
    "        frame_buffer.append(frame)\n",
    "        \n",
    "        # Predict every `interval` seconds\n",
    "        if time.time() - last_pred_time >= interval:\n",
    "            if len(frame_buffer) >= window_size:\n",
    "                input_data = np.expand_dims(np.array(frame_buffer[-window_size:]), axis=0)  # Last `window_size` frames\n",
    "                pred = model.predict(input_data, verbose=0)\n",
    "                class_idx = np.argmax(pred)\n",
    "                predicted_label = encoder.inverse_transform([class_idx])[0]\n",
    "\n",
    "                # Display prediction with timestamp\n",
    "                timestamp = frame_idx / fps\n",
    "                minutes = int(timestamp // 60)\n",
    "                seconds = int(timestamp % 60)\n",
    "                print(f\"[{minutes:02d}:{seconds:02d}] Predicted Action: {predicted_label}\")\n",
    "\n",
    "                last_pred_time = time.time()\n",
    "        \n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "# Load saved model\n",
    "model = tf.keras.models.load_model(\"goat_action_3dcnn.h5\")\n",
    "\n",
    "# Initialize encoder with class labels\n",
    "CLASSES = ['Running', 'Sitting', 'Grazing']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(CLASSES)\n",
    "\n",
    "# Path to long video\n",
    "video_path = r\"stitched_output.mp4\"\n",
    "\n",
    "# Predict actions in real-time every 5 seconds\n",
    "predict_real_time(video_path, model, encoder, window_size=16, stride=8, img_size=(64, 64), interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: stitched_output.mp4\n",
      "Total frames: 4540, FPS: 41.91179785039039\n",
      "Frames per 5-second window: 209, Sampling 16 frames\n",
      "[00:05] Predicted Action: Grazing\n",
      "[00:10] Predicted Action: Grazing\n",
      "[00:15] Predicted Action: Grazing\n",
      "[00:20] Predicted Action: Grazing\n",
      "[00:25] Predicted Action: Grazing\n",
      "[00:30] Predicted Action: Running\n",
      "[00:35] Predicted Action: Grazing\n",
      "[00:40] Predicted Action: Grazing\n",
      "[00:45] Predicted Action: Grazing\n",
      "[00:50] Predicted Action: Grazing\n",
      "[00:55] Predicted Action: Grazing\n",
      "[01:00] Predicted Action: Running\n",
      "[01:05] Predicted Action: Sitting\n",
      "[01:10] Predicted Action: Sitting\n",
      "[01:15] Predicted Action: Sitting\n",
      "[01:20] Predicted Action: Grazing\n",
      "[01:25] Predicted Action: Grazing\n",
      "[01:30] Predicted Action: Grazing\n",
      "[01:35] Predicted Action: Grazing\n",
      "[01:40] Predicted Action: Grazing\n",
      "[01:45] Predicted Action: Grazing\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "\n",
    "def predict_real_time(video_path, model, encoder, img_size=(64, 64), interval=5, window_size=16):\n",
    "    \"\"\"\n",
    "    Predicts actions every `interval` seconds using 16 evenly spaced frames from the past 5 seconds.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_buffer = []\n",
    "    frame_idx = 0\n",
    "    last_pred_time = 0  # Last prediction time in video seconds\n",
    "\n",
    "    frames_needed = int(interval * fps)  # Total frames in a 5-second window\n",
    "\n",
    "    print(f\"Processing video: {video_path}\")\n",
    "    print(f\"Total frames: {total_frames}, FPS: {fps}\")\n",
    "    print(f\"Frames per {interval}-second window: {frames_needed}, Sampling {window_size} frames\")\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize and normalize frame\n",
    "        frame = cv2.resize(frame, img_size)\n",
    "        frame = frame / 255.0\n",
    "        frame_buffer.append(frame)\n",
    "\n",
    "        # Keep only the last `frames_needed` frames\n",
    "        if len(frame_buffer) > frames_needed:\n",
    "            frame_buffer.pop(0)  # Remove the oldest frame\n",
    "\n",
    "        # Compute current video time\n",
    "        current_time = frame_idx / fps  # Convert frame index to seconds\n",
    "\n",
    "        # Predict every `interval` seconds in video time\n",
    "        if current_time >= last_pred_time + interval:\n",
    "            if len(frame_buffer) == frames_needed:  # Ensure we have enough frames\n",
    "                # evenly spaced frames\n",
    "                sampled_frames = np.array([frame_buffer[math.floor(i * len(frame_buffer) / window_size)] for i in range(window_size)])\n",
    "\n",
    "                # Add batch dimension\n",
    "                input_data = np.expand_dims(sampled_frames, axis=0)\n",
    "                pred = model.predict(input_data, verbose=0)\n",
    "                class_idx = np.argmax(pred)\n",
    "                predicted_label = encoder.inverse_transform([class_idx])[0]\n",
    "\n",
    "                # Display prediction with timestamp\n",
    "                minutes = int(current_time // 60)\n",
    "                seconds = int(current_time % 60)\n",
    "                print(f\"[{minutes:02d}:{seconds:02d}] Predicted Action: {predicted_label}\")\n",
    "\n",
    "                last_pred_time = current_time  # Update last prediction time\n",
    "        \n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "# Load saved model\n",
    "model = tf.keras.models.load_model(\"goat_action_3dcnn.h5\")\n",
    "\n",
    "# Initialize encoder with class labels\n",
    "CLASSES = ['Running', 'Sitting', 'Grazing']\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(CLASSES)\n",
    "\n",
    "# Path to long video\n",
    "video_path = r\"stitched_output.mp4\"\n",
    "\n",
    "# Predict actions every 5 seconds using 16 sampled frames\n",
    "predict_real_time(video_path, model, encoder, img_size=(64, 64), interval=5, window_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize GUI - Di ko pa alam kung paano to gagawin. HAHAHAH\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "import tkinter as tk\n",
    "import customtkinter as ctk\n",
    "from PIL import Image, ImageTk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class GoatActionApp:\n",
    "    def __init__(self, root, model, encoder, img_size=(216, 216), interval=15, window_size=20):\n",
    "        self.root = root\n",
    "        self.root.title(\"Goat Action Recognition\")\n",
    "        self.model = model\n",
    "        self.encoder = encoder\n",
    "        self.img_size = img_size\n",
    "        self.interval = interval\n",
    "        self.window_size = window_size\n",
    "        self.frame_buffer = []\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.fps = self.cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "        self.frame_idx = 0\n",
    "        self.last_pred_time = 0\n",
    "        self.predicted_label = \"Waiting...\"\n",
    "        self.image_tk = None  # Store reference to prevent garbage collection\n",
    "        \n",
    "        # Create GUI layout\n",
    "        self.root.geometry(\"800x600\")\n",
    "        self.root.resizable(False, False)\n",
    "        \n",
    "        # Webcam frame\n",
    "        self.video_frame = ctk.CTkLabel(self.root, text=\"\", width=600, height=450)\n",
    "        self.video_frame.place(relx=0.5, rely=0.4, anchor=\"center\")\n",
    "        \n",
    "        # Prediction label\n",
    "        self.prediction_label = ctk.CTkLabel(self.root, text=self.predicted_label, font=(\"Arial\", 20))\n",
    "        self.prediction_label.place(relx=0.5, rely=0.85, anchor=\"center\")\n",
    "        \n",
    "        self.update_video()\n",
    "\n",
    "    def update_video(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            self.root.after(10, self.update_video)\n",
    "            return\n",
    "        \n",
    "        # Resize for prediction\n",
    "        frame_resized = cv2.resize(frame, self.img_size) / 255.0\n",
    "        self.frame_buffer.append(frame_resized)\n",
    "        \n",
    "        # Keep only the last required frames\n",
    "        frames_needed = int(self.interval * self.fps)\n",
    "        if len(self.frame_buffer) > frames_needed:\n",
    "            self.frame_buffer.pop(0)\n",
    "        \n",
    "        # Compute current time\n",
    "        current_time = self.frame_idx / self.fps\n",
    "        \n",
    "        if current_time >= self.last_pred_time + self.interval:\n",
    "            if len(self.frame_buffer) >= self.window_size:\n",
    "                sampled_frames = np.array([\n",
    "                    self.frame_buffer[math.floor(i * len(self.frame_buffer) / self.window_size)]\n",
    "                    for i in range(self.window_size)\n",
    "                ])\n",
    "                input_data = np.expand_dims(sampled_frames, axis=0)\n",
    "                pred = self.model.predict(input_data, verbose=0)\n",
    "                class_idx = np.argmax(pred)\n",
    "                self.predicted_label = self.encoder.inverse_transform([class_idx])[0]\n",
    "                self.prediction_label.configure(text=f\"Predicted Action: {self.predicted_label}\")\n",
    "                self.last_pred_time += self.interval\n",
    "        \n",
    "        # Convert to Tkinter format\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (600, 450))\n",
    "        self.image_tk = ImageTk.PhotoImage(Image.fromarray(frame))  # Store reference\n",
    "        self.video_frame.configure(image=self.image_tk)\n",
    "        self.video_frame.image = self.image_tk\n",
    "        \n",
    "        self.frame_idx += 1\n",
    "        self.root.after(10, self.update_video)\n",
    "    \n",
    "    def run(self):\n",
    "        self.root.mainloop()\n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = tf.keras.models.load_model(\"examination_behavior-20F-216IS-16.keras\")\n",
    "    CLASSES = ['Suspicious', 'Non-suspicious']\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(CLASSES)\n",
    "    root = ctk.CTk()\n",
    "    app = GoatActionApp(root, model, encoder)\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize GUI - Di ko pa alam kung paano to gagawin. HAHAHAH\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import tkinter as tk\n",
    "import customtkinter as ctk\n",
    "from PIL import Image, ImageTk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class GoatActionApp:\n",
    "    def __init__(self, root, model, encoder, img_size=(216, 216), interval=15, window_size=20):\n",
    "        self.root = root\n",
    "        self.root.title(\"Goat Action Recognition\")\n",
    "        self.model = model\n",
    "        self.encoder = encoder\n",
    "        self.img_size = img_size\n",
    "        self.interval = interval\n",
    "        self.window_size = window_size\n",
    "        self.frame_buffer = []\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.fps = self.cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "        self.frame_idx = 0\n",
    "        self.last_pred_time = 0\n",
    "        self.predicted_label = \"Waiting...\"\n",
    "        self.image_tk = None\n",
    "        self.seconds_elapsed = 1  # Start from 1\n",
    "        \n",
    "        # GUI Layout\n",
    "        self.root.geometry(\"800x600\")\n",
    "        self.root.resizable(False, False)\n",
    "        \n",
    "        self.video_frame = ctk.CTkLabel(self.root, text=\"\", width=600, height=450)\n",
    "        self.video_frame.place(relx=0.5, rely=0.4, anchor=\"center\")\n",
    "        \n",
    "        self.prediction_label = ctk.CTkLabel(self.root, text=self.predicted_label, font=(\"Arial\", 20))\n",
    "        self.prediction_label.place(relx=0.5, rely=0.85, anchor=\"center\")\n",
    "\n",
    "        self.timer_label = ctk.CTkLabel(self.root, text=f\"Time since last prediction: {self.seconds_elapsed}s\", font=(\"Arial\", 16))\n",
    "        self.timer_label.place(relx=0.5, rely=0.92, anchor=\"center\")\n",
    "        \n",
    "        self.update_video()\n",
    "        self.update_timer()\n",
    "\n",
    "    def update_video(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            self.root.after(10, self.update_video)\n",
    "            return\n",
    "        \n",
    "        frame_resized = cv2.resize(frame, self.img_size) / 255.0\n",
    "        self.frame_buffer.append(frame_resized)\n",
    "        \n",
    "        frames_needed = int(self.interval * self.fps)\n",
    "        if len(self.frame_buffer) > frames_needed:\n",
    "            self.frame_buffer.pop(0)\n",
    "        \n",
    "        current_time = self.frame_idx / self.fps\n",
    "\n",
    "        if current_time >= self.last_pred_time + self.interval:\n",
    "            if len(self.frame_buffer) >= self.window_size:\n",
    "                sampled_frames = np.array([\n",
    "                    self.frame_buffer[math.floor(i * len(self.frame_buffer) / self.window_size)]\n",
    "                    for i in range(self.window_size)\n",
    "                ])\n",
    "                input_data = np.expand_dims(sampled_frames, axis=0)\n",
    "                pred = self.model.predict(input_data, verbose=0)\n",
    "                class_idx = np.argmax(pred)\n",
    "                self.predicted_label = self.encoder.inverse_transform([class_idx])[0]\n",
    "                self.prediction_label.configure(text=f\"Predicted Action: {self.predicted_label}\")\n",
    "                self.last_pred_time += self.interval\n",
    "                self.seconds_elapsed = 1  # Reset to 1\n",
    "\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (600, 450))\n",
    "        self.image_tk = ImageTk.PhotoImage(Image.fromarray(frame))\n",
    "        self.video_frame.configure(image=self.image_tk)\n",
    "        self.video_frame.image = self.image_tk\n",
    "        \n",
    "        self.frame_idx += 1\n",
    "        self.root.after(10, self.update_video)\n",
    "    \n",
    "    def update_timer(self):\n",
    "        if self.seconds_elapsed < self.interval:\n",
    "            self.seconds_elapsed += 1\n",
    "        self.timer_label.configure(text=f\"Time since last prediction: {self.seconds_elapsed}s\")\n",
    "        self.root.after(1000, self.update_timer)\n",
    "\n",
    "    def run(self):\n",
    "        self.root.mainloop()\n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = tf.keras.models.load_model(\"examination_behavior-20F-216IS-16.keras\")\n",
    "    CLASSES = ['Suspicious', 'Non-suspicious']\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(CLASSES)\n",
    "    root = ctk.CTk()\n",
    "    app = GoatActionApp(root, model, encoder)\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n",
      "c:\\Users\\Raymond\\Desktop\\3D-CNN\\venv\\lib\\site-packages\\customtkinter\\windows\\widgets\\core_widget_classes\\ctk_base_class.py:179: UserWarning: CTkLabel Warning: Given image is not CTkImage but <class 'PIL.ImageTk.PhotoImage'>. Image can not be scaled on HighDPI displays, use CTkImage instead.\n",
      "\n",
      "  warnings.warn(f\"{type(self).__name__} Warning: Given image is not CTkImage but {type(image)}. Image can not be scaled on HighDPI displays, use CTkImage instead.\\n\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring logs saved to: monitoring_logs\\monitoring_log_20250428-153629.csv\n",
      "Landmark logs saved to: monitoring_logs\\landmarks_log_20250428-153629.csv\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import tkinter as tk\n",
    "import customtkinter as ctk\n",
    "from PIL import Image, ImageTk\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "class BehaviorMonitoringApp:\n",
    "    def __init__(self, root, model, encoder, img_size=(216, 216), interval=15, window_size=20):\n",
    "        self.root = root\n",
    "        self.root.title(\"Student Behavior Monitoring\")\n",
    "        self.model = model\n",
    "        self.encoder = encoder\n",
    "        self.img_size = img_size\n",
    "        self.interval = interval\n",
    "        self.window_size = window_size\n",
    "        self.frame_buffer = []\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.fps = self.cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "        self.frame_idx = 0\n",
    "        self.last_pred_time = 0\n",
    "        self.predicted_label = \"Waiting...\"\n",
    "        self.image_tk = None\n",
    "        self.seconds_elapsed = 1\n",
    "\n",
    "        # Initialize CSV saving\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        self.save_folder = \"monitoring_logs\"\n",
    "        os.makedirs(self.save_folder, exist_ok=True)\n",
    "        self.monitor_csv_path = os.path.join(self.save_folder, f\"monitoring_log_{timestamp}.csv\")\n",
    "        self.landmark_csv_path = os.path.join(self.save_folder, f\"landmarks_log_{timestamp}.csv\")\n",
    "        self.logs = []\n",
    "        self.landmark_logs = []\n",
    "\n",
    "        # GUI Layout\n",
    "        self.root.geometry(\"800x600\")\n",
    "        self.root.resizable(False, False)\n",
    "        \n",
    "        self.video_frame = ctk.CTkLabel(self.root, text=\"\", width=600, height=450)\n",
    "        self.video_frame.place(relx=0.5, rely=0.4, anchor=\"center\")\n",
    "        \n",
    "        self.prediction_label = ctk.CTkLabel(self.root, text=self.predicted_label, font=(\"Arial\", 20))\n",
    "        self.prediction_label.place(relx=0.5, rely=0.85, anchor=\"center\")\n",
    "\n",
    "        self.timer_label = ctk.CTkLabel(self.root, text=f\"Time since last prediction: {self.seconds_elapsed}s\", font=(\"Arial\", 16))\n",
    "        self.timer_label.place(relx=0.5, rely=0.92, anchor=\"center\")\n",
    "        \n",
    "        self.update_video()\n",
    "        self.update_timer()\n",
    "\n",
    "    def update_video(self):\n",
    "        ret, frame = self.cap.read()\n",
    "        if not ret:\n",
    "            self.root.after(10, self.update_video)\n",
    "            return\n",
    "        \n",
    "        frame_resized = cv2.resize(frame, self.img_size) / 255.0\n",
    "        self.frame_buffer.append(frame_resized)\n",
    "        \n",
    "        frames_needed = int(self.interval * self.fps)\n",
    "        if len(self.frame_buffer) > frames_needed:\n",
    "            self.frame_buffer.pop(0)\n",
    "        \n",
    "        current_time = self.frame_idx / self.fps\n",
    "\n",
    "        if current_time >= self.last_pred_time + self.interval:\n",
    "            if len(self.frame_buffer) >= self.window_size:\n",
    "                sampled_frames = np.array([\n",
    "                    self.frame_buffer[math.floor(i * len(self.frame_buffer) / self.window_size)]\n",
    "                    for i in range(self.window_size)\n",
    "                ])\n",
    "                input_data = np.expand_dims(sampled_frames, axis=0)  # (1, 20, 216, 216, 3)\n",
    "                \n",
    "                # Multitask model output: 4 heads\n",
    "                classification_pred, landmarks_pred, motion_pred, velocity_pred = self.model.predict(input_data, verbose=0)\n",
    "\n",
    "                # --- CLASSIFICATION ---\n",
    "                class_idx = np.argmax(classification_pred[0])\n",
    "                self.predicted_label = self.encoder.inverse_transform([class_idx])[0]\n",
    "                self.prediction_label.configure(text=f\"Predicted: {self.predicted_label}\")\n",
    "\n",
    "                # --- SAVE LANDMARKS for 20 FRAMES CLEANLY (X, Y separate) ---\n",
    "                landmarks = landmarks_pred[0]  # (142,) → (71 points × 2 coordinates)\n",
    "                landmarks = landmarks.reshape(71, 2)  # (71, 2)\n",
    "\n",
    "                time_now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                for idx in range(self.window_size):\n",
    "                    frame_landmark_entry = {\n",
    "                        \"Time\": time_now,\n",
    "                        \"Frame_Index\": idx\n",
    "                    }\n",
    "                    for point_idx, (x, y) in enumerate(landmarks):\n",
    "                        frame_landmark_entry[f\"Landmark_{point_idx}_X\"] = x\n",
    "                        frame_landmark_entry[f\"Landmark_{point_idx}_Y\"] = y\n",
    "                    self.landmark_logs.append(frame_landmark_entry)\n",
    "\n",
    "                # --- Save Monitoring Log (Motion, Velocity) ---\n",
    "                motion = motion_pred[0]\n",
    "                velocity = velocity_pred[0]\n",
    "                log_entry = {\n",
    "                    \"Time\": time_now,\n",
    "                    \"Prediction\": self.predicted_label\n",
    "                }\n",
    "                for i in range(len(motion)):\n",
    "                    log_entry[f\"Motion_{i}\"] = motion[i]\n",
    "                    log_entry[f\"Velocity_{i}\"] = velocity[i]\n",
    "                self.logs.append(log_entry)\n",
    "\n",
    "                self.last_pred_time += self.interval\n",
    "                self.seconds_elapsed = 1\n",
    "\n",
    "        # Update GUI\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame = cv2.resize(frame, (600, 450))\n",
    "        self.image_tk = ImageTk.PhotoImage(Image.fromarray(frame))\n",
    "        self.video_frame.configure(image=self.image_tk)\n",
    "        self.video_frame.image = self.image_tk\n",
    "        \n",
    "        self.frame_idx += 1\n",
    "        self.root.after(10, self.update_video)\n",
    "    \n",
    "    def update_timer(self):\n",
    "        if self.seconds_elapsed < self.interval:\n",
    "            self.seconds_elapsed += 1\n",
    "        self.timer_label.configure(text=f\"Time since last prediction: {self.seconds_elapsed}s\")\n",
    "        self.root.after(1000, self.update_timer)\n",
    "\n",
    "    def run(self):\n",
    "        self.root.mainloop()\n",
    "        self.cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # Save logs to CSVs\n",
    "        if self.logs:\n",
    "            df_monitor = pd.DataFrame(self.logs)\n",
    "            df_monitor.to_csv(self.monitor_csv_path, index=False)\n",
    "            print(f\"Monitoring logs saved to: {self.monitor_csv_path}\")\n",
    "\n",
    "        if self.landmark_logs:\n",
    "            df_landmarks = pd.DataFrame(self.landmark_logs)\n",
    "            df_landmarks.to_csv(self.landmark_csv_path, index=False)\n",
    "            print(f\"Landmark logs saved to: {self.landmark_csv_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = tf.keras.models.load_model(\"models/examination_behavior-20F-216IS-v1.05.keras\")\n",
    "    CLASSES = ['Non-suspicious', 'Suspicious']\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(CLASSES)\n",
    "    root = ctk.CTk()\n",
    "    app = BehaviorMonitoringApp(root, model, encoder)\n",
    "    app.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (36, 20, 216, 216, 3), Train labels: (36, 2)\n"
     ]
    }
   ],
   "source": [
    "#load with memory\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = 216  # Resize frames to 64x64\n",
    "FRAMES = 20  # Fixed number of frames per video\n",
    "CHANNELS = 3  # RGB channels\n",
    "CLASSES = ['Suspicious', 'Non-suspicious']\n",
    "\n",
    "# Function to load videos and convert them to arrays\n",
    "def load_videos_from_folder(folder_path, class_labels):\n",
    "    data, labels = [], []\n",
    "    \n",
    "    for class_name in class_labels:\n",
    "        class_path = os.path.join(folder_path, class_name)\n",
    "        \n",
    "        for video_name in os.listdir(class_path):\n",
    "            video_path = os.path.join(class_path, video_name)\n",
    "            frames = extract_video_frames(video_path, FRAMES, IMG_SIZE)\n",
    "            \n",
    "            if frames is not None:\n",
    "                data.append(frames)\n",
    "                labels.append(class_name)\n",
    "    \n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Function to extract fixed number of frames from a video\n",
    "def extract_video_frames(video_path, num_frames, img_size):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "    \n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)  # Select evenly spaced frames\n",
    "    \n",
    "    for i in range(total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        if i in frame_indices:\n",
    "            frame = cv2.resize(frame, (img_size, img_size))\n",
    "            frames.append(frame)\n",
    "    \n",
    "    cap.release()\n",
    "    \n",
    "    if len(frames) < num_frames:  # Pad if not enough frames\n",
    "        frames.extend([frames[-1]] * (num_frames - len(frames)))\n",
    "    \n",
    "    return np.array(frames) if len(frames) == num_frames else None\n",
    "\n",
    "# Load datasets\n",
    "test_videos, test_labels = load_videos_from_folder(\"dataset/test\", CLASSES)\n",
    "\n",
    "# Normalize data\n",
    "test_videos = test_videos / 255.0\n",
    "\n",
    "# Encode labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(test_labels)  # Fit before transform\n",
    "test_labels_enc = to_categorical(encoder.transform(test_labels))\n",
    "\n",
    "\n",
    "print(f\"Train shape: {test_videos.shape}, Train labels: {test_labels_enc.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model(\"examination_behavior-20F-216IS-16.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 384ms/step\n",
      "Predicted Action: Non-suspicious\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378ms/step\n",
      "Predicted Action: Suspicious\n"
     ]
    }
   ],
   "source": [
    "CLASSES = ['Non-suspicious', 'Suspicious']\n",
    "\n",
    "\n",
    "def predict_video(video_path, model, encoder):\n",
    "    video_data = extract_video_frames(video_path, FRAMES, IMG_SIZE)\n",
    "    \n",
    "    if video_data is None:\n",
    "        print(\"Error processing video.\")\n",
    "        return\n",
    "    \n",
    "    video_data = np.expand_dims(video_data / 255.0, axis=0)\n",
    "    prediction = model.predict(video_data)\n",
    "    \n",
    "    class_idx = np.argmax(prediction)\n",
    "    predicted_label = encoder.inverse_transform([class_idx])[0]\n",
    "    print(f\"Predicted Action: {predicted_label}\")\n",
    "\n",
    "# Test with a sample video\n",
    "predict_video(r\"dataset\\test\\Non-suspicious\\Video 04-082.mp4\", model, encoder)\n",
    "predict_video(r\"dataset\\test\\Suspicious\\Video 04-037.mp4\", model, encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_videos_in_folder(folder_path, model, encoder):\n",
    "    if model is None or encoder is None:\n",
    "        print(\"Model or encoder not provided.\")\n",
    "        return\n",
    "\n",
    "    for class_folder in os.listdir(folder_path):\n",
    "        class_path = os.path.join(folder_path, class_folder)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "\n",
    "        for video_name in os.listdir(class_path):\n",
    "            video_path = os.path.join(class_path, video_name)\n",
    "            if not video_name.lower().endswith((\".mp4\", \".avi\", \".mov\", \".mkv\")):\n",
    "                continue\n",
    "\n",
    "            video_data = extract_video_frames(video_path, FRAMES, IMG_SIZE)\n",
    "            if video_data is None:\n",
    "                print(f\"[SKIPPED] {video_name}: error extracting frames.\")\n",
    "                continue\n",
    "\n",
    "            video_data = np.expand_dims(video_data / 255.0, axis=0)\n",
    "            prediction = model.predict(video_data)\n",
    "            class_idx = np.argmax(prediction)\n",
    "            predicted_label = encoder.inverse_transform([class_idx])[0]\n",
    "\n",
    "            print(f\"{video_name} => Predicted: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_videos_in_folder(\"dataset/test\", model, encoder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
